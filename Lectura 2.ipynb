{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c02fd459",
   "metadata": {},
   "source": [
    "# Lectura 2 - Regresión lineal con una variable\n",
    "\n",
    "## RafaCastle\n",
    "\n",
    "## 2.1 Representación de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2a9eba",
   "metadata": {},
   "source": [
    "### Definiciones\n",
    "\n",
    "Sea $C$ un conjunto de datos, tal que $C = \\{(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), ... , (x^{(M)},y^{(M)}) \\}$, se denota:\n",
    "\n",
    "$X = (x^{(1)}, x^{(2)}, ... , x^{(M)})$ la variable input\n",
    "\n",
    "$Y = (y^{(1)}, y^{(2)}, ... , y^{(M)})$ la variable target o output\n",
    "\n",
    "$h: X \\to Y$ la función hipótesis, que se representará en esta lectura como $h_\\theta(x) = \\theta_o + \\theta_1 x$\n",
    "\n",
    "La función hipótesis se construye con un subconjunto $T \\subset C$ de $m$ elementos y busca estimar los valores de $Y$ lo mejor posible. No necesariamente $h$ debe ser una función lineal, solo se considerará lineal para el inicio de está lectura."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb30937",
   "metadata": {},
   "source": [
    "## 2.2 Cost Function\n",
    "\n",
    "Dada la forma de $h$, ¿Cómo se escogen los valores $\\theta_o$ y $\\theta_1$ para un cierto conjunto de datos?, es claro que se deben escoger de modo que $h_\\theta(x)$ sea próximo a $y$ para un elemento $(x,y) \\in T$, por lo que se busca minimizar la siguiente expresión, denominada Cost Function $J$:\n",
    "\n",
    "$$\n",
    "J(\\theta_o, \\theta_1) = \\frac{1}{2m}\\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)} \\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0ec19e",
   "metadata": {},
   "source": [
    "## 2.3 Intuición sobre Cost Function #1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411f73c8",
   "metadata": {},
   "source": [
    "Para simplificar el problema es posible tomar $\\theta_o = 0$, de modo que $h_\\theta (x) = \\theta_1 x$, por lo que se buscaría minimizar la función $J (\\theta_1)$. En la siguiente gráfica se muestra como varía la función $J (\\theta_1)$ al variar $\\theta_1$ para un cierto conjunto de datos.\n",
    "\n",
    "![Title](Imágenes/2-3-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a761fade",
   "metadata": {},
   "source": [
    "La gráfica muestra una parábola cn vértice en $\\theta_1 = 1$, por lo que ese valor es el que minimiza la función $J$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db78c92",
   "metadata": {},
   "source": [
    "## 2.4 Intuición sobre Cost Function #2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b94940a",
   "metadata": {},
   "source": [
    "Retomando el problema original, donde $\\theta_o, \\theta_1 \\neq 0$, la gráfica de $J(\\theta_o, \\theta_1)$ sería tridimensional, como se muestra en la siguiente figura:\n",
    "\n",
    "![Title](Imágenes\\2-4-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49da8f2",
   "metadata": {},
   "source": [
    "Es posible también ver las curvas de nivel de la función $J$ en 2 dimensiones, cada curva depresenta un área en el espacio $(\\theta_1, \\theta_o)$ donde la función $J$ tiene el mismo valor.\n",
    "\n",
    "![Title](Imágenes/2-4-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da12a21",
   "metadata": {},
   "source": [
    "En este caso se busca también minimizara $J$, para la gráfica en 3D es claro que el valor mínimo es el punto de mínima altura en la gráfica, para las curvas de nivel, el valor mínimo de $J$ se encuentra en el vértice de las elipses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda3ce6e",
   "metadata": {},
   "source": [
    "## 2.5 Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91627f0d",
   "metadata": {},
   "source": [
    "Gradient descent es el algoritmo más general para minimizar el valor de la función $J(\\theta_1, \\theta_o)$, de hecho es un algoritmo que sirve para una función $J(\\theta_o, \\theta_1, ... , \\theta_n)$, por ahora se considerará solo el caso $n=1$. \n",
    "\n",
    "El algoritmo parte de una posición inicial y encuentra un valor mínimo en $J$, de manera intuitiva el algoritmo funciona como se muestra en la siguiente figura:\n",
    "\n",
    "![Title](Imágenes\\2-5-1.png)\n",
    "\n",
    "En la figura se observa que el algorimto encontró 2 mínimos diferentes para el valor de $J$ porque partió de 2 posiciones iniciales diferentes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b7471b",
   "metadata": {},
   "source": [
    "### Algoritmo\n",
    "\n",
    "El siguiente algoritmo se repite hasta la convergencia para $j=0,1$\n",
    "\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J (\\theta_o, \\theta_1)\n",
    "$$\n",
    "\n",
    "o de manera computacional puede verse como:\n",
    "\n",
    "![Title](Imágenes\\2-5-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d948ef",
   "metadata": {},
   "source": [
    "El valor de $\\alpha$ determina el tamaño del paso, en la siguiente sección se profundizará más sobre este parámetro."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dc3b0c",
   "metadata": {},
   "source": [
    "## 2.6 Intuición sobre Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f08c7ed",
   "metadata": {},
   "source": [
    "El valor de $\\alpha$ debe escogerse con cuidado para evitar los siguientes errores:  \n",
    "\n",
    "1. Un valor de $\\alpha$ muy pequeño: El algoritmo puede tardarse demasiado en converger, pero el valor hallado será más preciso, como se muestra en la primera gráfica.\n",
    "\n",
    "2. Un valor de $\\alpha$ muy grande: Se aproximará más rápido se aproximará al valor mínimo de $J$ pero cuidado, el algoritmo se puede saltar el valor mínimo de $J$, como se muestra en la segunda gráfica.\n",
    "\n",
    "![Title](Imágenes\\2-6-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c3c022",
   "metadata": {},
   "source": [
    "Es claro que la manera más eficiente de obtener a $\\alpha$ es que sea un valor variable, que sea grande cuando esté lejos del mínimo pero conforme se acerca a él vaya reduciendo su valor, osea, que directamente proporcional a la magnitud de la pendiente en un punto de la gráfica de $J$, como se muestra en la figura:\n",
    "\n",
    "![Title](Imágenes\\2-6-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376d75ff",
   "metadata": {},
   "source": [
    "## 2.7 Gradient descent para la regresión lineal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df966b3f",
   "metadata": {},
   "source": [
    "Al aplicar el algoritmo gradient descent a la regresión lineal tenemos:\n",
    "    \n",
    "$\\frac{\\partial}{\\partial \\theta_o} J (\\theta_o, \\theta_1) = \\frac{1}{m} \\sum_{i=1}^m (h_\\theta (x^{(i)})-y^{(i)})$ para $j=0$\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\theta_1} J (\\theta_o, \\theta_1) = \\frac{1}{m} \\sum_{i=1}^m (h_\\theta (x^{(i)})-y^{(i)}) \\cdot x^{(i)} $ para $j=1$\n",
    "\n",
    "por lo tanto, para obtener a los valores $(\\theta_o, \\theta_1)$ se debe repetir hasta la convergencia el siguiente algoritmo:\n",
    "\n",
    "$$\\theta_o := \\theta_o - \\alpha \\frac{1}{m} \\sum_{i=1}^m (h_\\theta (x^{(i)})-y^{(i)})$$\n",
    "\n",
    "$$\\theta_1 := \\theta_1 - \\alpha \\frac{1}{m} \\sum_{i=1}^m (h_\\theta (x^{(i)})-y^{(i)}) \\cdot x^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad76fe13",
   "metadata": {},
   "source": [
    "Gráficamente así actúa el algoritmo en un espació $(\\theta_o, \\theta_1)$, aproximándose al vértice de las elipses, que corresponde al valor mínimo de $J$.\n",
    "\n",
    "![Title](Imágenes\\2-7-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0f4544",
   "metadata": {},
   "source": [
    "## 2.8 ¿Qué sigue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc0c3a9",
   "metadata": {},
   "source": [
    "Existen 2 extenciones a este algoritmo:\n",
    "\n",
    "1. Se puede hallar $\\theta_o$ y $\\theta_1$ de manera exacta, sin un algoritmo iterativo.\n",
    "2. Se puede realizar un modelo con una mayor cantidad de atributos, no solo 2.\n",
    "\n",
    "Para ampliar este algorimto se usará algebra linea, la lectura 3 del curso de Andrew Ng es un repaso de algebra lineal del cual no realizaré notas a menos que se toque algún tema puntual de Machine Learning. Si el lector no está familiarizado con temas básicos de algebra lineal recomiendo buscar la lectura 3 de Andrew Ng en YouTube antes de continuar con la lectura 4 en estas notas. Los temas que se ven en su repaso son:\n",
    "\n",
    " - Vectores y matrices\n",
    " - Operaciones básicas con vectores y matrices\n",
    " - Matrices inversas y transpuestas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

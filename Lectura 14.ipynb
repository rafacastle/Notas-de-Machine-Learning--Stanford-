{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d4deb5d",
   "metadata": {},
   "source": [
    "# Lectura 14\n",
    "## RafaCastle\n",
    "## 14.1 Reducción de dimensionalidad - Compresión de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0241d5d6",
   "metadata": {},
   "source": [
    "La compresión de los datos no solo ayuda a disminuir el espacio ocupado en el ordenador, sino también a acelerar el funcionamiento de los algoritmos.\n",
    "\n",
    "Supongamos que se tiene un conjunto de datos en 2 dimensiones, de modo que sus 2 variables están fuertemente relacionadas entre sí. En dicho caso es posible hacer un mapeo, es decir, supongamos que uno de los puntos en $R^2$ es $x^{(1)} = (x_1,x_2)$. En dicho caso, puede definirse una nueva variable $z_1 \\in R$ tal que $x^{(1)} \\to z_1$. \n",
    "\n",
    "![Title](Imágenes/14-1-1.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c7fbb66f",
   "metadata": {},
   "source": [
    "De manera análoga se puede hacer lo mismo para mapear un conjunto de datos de $R^3$ a $R^2$. Se proyecta el conjunto de datos a un plano para poder identificar a un objeto solo mediante 2 parámetros, es decir.\n",
    "\n",
    "Si se tenía $x^{(1)} = (x_1, x_2, x_3)$ se define un nuevo vector $z^{(1)} = (z_1,z_2)$ tal que $x^{(1)} \\to z^{(1)}$\n",
    "\n",
    "![Title](Imágenes/14-1-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42e426f",
   "metadata": {},
   "source": [
    "## 14.2 Motivación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e0683e",
   "metadata": {},
   "source": [
    "No me parece que esta sección sea muy útil ya que solo se ilustra de manera cualitativa como un conjunto con 50 atributos se reduce a un conjunto de 2. Dejo el link del vídeo [aquí](https://www.youtube.com/watch?v=Zbr5hyJNGCs&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=81)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861940a6",
   "metadata": {},
   "source": [
    "## 14.3 Análisis de la componente principal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab09deb5",
   "metadata": {},
   "source": [
    "El algoritmo PCA (principal component analysis) es uno de los más utilizados para la reducción de la dimensionalidad. Supongamos que se tiene el siguiente conjunto de datos, el algoritmo PCA proyecta dichos puntos a un ajuste lineal, de modo que la suma de los cuadrados de las ditancias sea mínima:\n",
    "\n",
    "![Title](Imágenes/14-3-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7979385c",
   "metadata": {},
   "source": [
    "A la suma de los cuadrados de las distancias de los puntos al ajuste se le denomina proyección de error, de modo que el algoritmo puede resumirse a encontrar un vector hacia el cual, al proyectar los datos se tiene que la proyección de error sea mínima. Para reducir de una dimensión $n$ a una dimensión $k$ se deben encontrar $k$ vectores de modo que la proyección de error de los datos hacia el espacio generado por esos vectores sea mínima. Por ejemplo, si se desea reducir un conjunto de datos en $R^3$ a $R^2$, se hayan 2 vectores en $R^3$ que generen un plano. La distancia de los puntos en $R^3$ a dicho plano es la proyección de error y el plano sería el espacio mencionado previamente.\n",
    "\n",
    "Tras todo lo mencionado anteriormente, es normal pensar que el algoritmo PCA es prácticamente igual al algoritmo de regresión lineal. Sin embargo esto no es así, la diferencia principalmente radica en que, en regresión lineal, la distancia que busca minimizarse es la distancia vertical del punto al ajuste, mientras que en el algoritmo PCA se busca minimizar la distancia ortogonal del punto al ajuste.\n",
    "\n",
    "![Title](Imágenes/14-3-2.png)\n",
    "\n",
    "Por otro lado el objetivo de los algoritmos es distinto, el algoritmo de regresión busca predecir la variable $y$, mientras que el de PCA busca reducir la dimensión del conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c440625",
   "metadata": {},
   "source": [
    "## 14.4 Algoritmo PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0dc6f7",
   "metadata": {},
   "source": [
    "Antes de aplicar el algoritmo PCA existe un preprocesamiento de los datos que debe ser tomado en cuenta. Si se tiene un conjunto de entrenamiento con $m$ datos, $\\{ x^{(1)}, x^{(2)}, ..., x^{(m)} \\}$. Y se cacula su media como:\n",
    "\n",
    "$$\n",
    "\\mu_j = \\frac{1}{m} \\sum_{i=1}^m x^{(i)}_j\n",
    "$$\n",
    "\n",
    "Así, se reemplaza cada $x^{(i)}_j$ con $x_j - \\mu_j$. De modo que la media del nuevo conjunto sea 0. Además también hay que escalar las variables de modo que tengan magnitudes similares. \n",
    "\n",
    "$$\n",
    "x_j^{(i)} \\to \\frac{x_j^{(i)} - \\mu_j}{s_j}\n",
    "$$\n",
    "\n",
    "Donde $s_j$ puede ser la desviación estándar o el valor máximo de $x_j$. \n",
    "\n",
    "Una vez hecho el preprocesamiento, el algoritmo hace lo descrito en las secciones anteriores, reduce la dimensión de los datos proyectandolos en un espacio de dimensión menor. Supongamos que se quiere reducir de una dimensión $n$ a una dimensión $k$. El algoritmo primero calcula la matriz de covarianzas:\n",
    "\n",
    "$$\n",
    "\\Sigma = \\frac{1}{m} \\sum_{i=1}^{n} (x^{(i)})(x^{(i)})^T\n",
    "$$\n",
    "\n",
    "Posteriormente se calculan los eigenvectores de $\\Sigma$, notemos que sigma es una matriz de dimensión $n \\times n$. Por lo que tiene $n$ eigenvectores que pueden escribirse en una matriz $U \\in R^{n \\times n}$. Podemos denotarla como:\n",
    "\n",
    "$$\n",
    "U = [u^{(1)}, u^{(2)},...,u^{(m)}]\n",
    "$$\n",
    "\n",
    "con $u^{(i)} \\in R^n$ eigenvector de $\\Sigma$, así, los vectores que generan el espaco de dimensión $k$ serían los primeros $k$ vectores de $U$, es decir $\\{u^{(1)},u^{(2)},...,u^{(k)}\\}$ (nota que no importa como esten ordenados los vectores $u^{(i)}$ en $U$). Denominamos entonces\n",
    "\n",
    "$$\n",
    "z^{(i)} := [u^{(1)},u^{(2)},...,u^{(k)}]^T x^{(i)} \\hspace{0.5cm} \\text{con} \\hspace{0.5cm} [u^{(1)},u^{(2)},...,u^{(k)}] \\in R^{n \\times k} \n",
    "$$\n",
    "\n",
    "Como $x^{(i)} \\in R^{n}$ entonces $z^{(i)}  \\in R^k$. Siendo $z^{(i)}$ el punto $x^{(i)}$ proyectado en el espacio reducido que está construido por los vectores $u^{(i)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43154e84",
   "metadata": {},
   "source": [
    "## 14.5 Eligiendo el número de componentes principales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26071dc1",
   "metadata": {},
   "source": [
    "Para escoger al número $k$ suelen usarse los siguientes conceptos:\n",
    "\n",
    "1. Promedio de el error de proyección al cuadrado $\\frac{1}{m} \\sum_{i=1}^m|| x^{(i)} - x^{(i)}_{\\text{app}} ||$\n",
    "2. Variación total de los datos $\\frac{1}{m} \\sum_{i=1}^m|| x^{(i)}||^2$\n",
    "\n",
    "Se busca el menor valor de $k$ que cumpla \n",
    "\n",
    "$$\n",
    "\\frac{\\frac{1}{m} \\sum_{i=1}^m|| x^{(i)} - x^{(i)}_{\\text{app}} ||}{\\frac{1}{m} \\sum_{i=1}^m|| x^{(i)}||^2} \\leq 0.01\n",
    "$$\n",
    "\n",
    "Notemos que la variable que depende de $k$ en las expresiones anteriores es indirectamente $ x^{(i)}_{\\text{app}}$. A veces puede variar el valor de la derecha a $0.05 \\%$ dependiendo que tanto quiera contenerse la varianza (a mayor valor, menos se retiene la varianza). \n",
    "\n",
    "Este método no es del todo eficiente cuando $k$ toma valores grandes, ya que hay que hacerlo una y otra vez hasta hallar el valor de $k$ indicado empezando desde 1. Existe otra manera de hallar el valor de $k$. Supongamos que la matriz $S$ contiene en su diagonal a todos los eigenvectores de la matriz $\\Sigma$, siendo los demás valores 0 :\n",
    "\n",
    "$$\n",
    "S = \\left[ \\begin{array}{ccccc}\n",
    "s_{11} & 0 & 0 & ... & 0 \\\\\n",
    "0 & s_{22} & 0 & ... & 0 \\\\\n",
    "0 & ... & ... & ... & 0 \\\\\n",
    "0 & 0 & 0 & ... & s_{nn} \\\\\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Así, dado un valor $k$ se cumple que:\n",
    "\n",
    "$$\n",
    "\\frac{\\frac{1}{m} \\sum_{i=1}^m|| x^{(i)} - x^{(i)}_{\\text{app}} ||}{\\frac{1}{m} \\sum_{i=1}^m|| x^{(i)}||^2} = 1- \\frac{\\sum_{i=1}^k S_{ii}}{\\sum_{i=1}^n S_{ii}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967a7099",
   "metadata": {},
   "source": [
    "Por lo que el problema se puede reducir a encontrar al valor más pequeño de $k$ que cumpla:\n",
    "\n",
    "$$\n",
    "\\frac{\\sum_{i=1}^k S_{ii}}{\\sum_{i=1}^n S_{ii}} \\ge 0.99\n",
    "$$\n",
    "\n",
    "Lo cual es mucho más eficiente. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b52d18d",
   "metadata": {},
   "source": [
    "## 14.6 Reconstrucción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b88d90",
   "metadata": {},
   "source": [
    "### ¿Cómo se regresa del espacio reducido al espacio original?\n",
    "\n",
    "Si se tiene un conjunto de datos $\\{x^{(1)},x^{(2)},...,x^{(m)}\\}$ y se proyectan a los puntos en el espacio reducido $\\{z^{(1)},z^{(2)},...,z^{(m)}\\}$, existe otro conjunto de puntos en el espacio original denominados $\\{x_{\\text{approx}}^{(1)},x_{\\text{approx}}^{(2)},...,x_{\\text{approx}}^{(m)}\\}$. De modo que $x^{(i)} \\approx x_{\\text{approx}}^{(i)}$, como se muestra en la figura:\n",
    "\n",
    "![Title](Imágenes/14-6-1.png)\n",
    "\n",
    "Al proceso de obtener a estos puntos aproximados se le denomina reconstrucción."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5435f5",
   "metadata": {},
   "source": [
    "## 14.7 Consejo sobre PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8019c3a2",
   "metadata": {},
   "source": [
    "El algoritmo PCA puede usarse para acelerar los algoritmos de aprendizaje supervizado ya que puede ofrecer la siguiente alternativa:\n",
    "\n",
    "Supongamos que se tiene un conjunto de datos $x^{(i)} \\in R^{10,000}$ y se quiere aplicar un algoritmo de aprendizaje supervizado, para reducir los requisitos computacionales, es posible reducir la dimensionalidad a un número menor, por ejemplo $x^{(i)} \\to z^{(i)} \\in R^{100}$ por medio de PCA. Tras obtener los resultados queridos en esta dimensión reducida, pueden reconstruirse los resultados como se mostró en la sección anterior.\n",
    "\n",
    "No es recomendable, utilizar este algoritmo para reducir el overfitting (aunque pueda parecer tentador), resulta mucho más útil y correcto aplicar una regularización en su lugar.\n",
    "\n",
    "Antes de usar PCA es también recomendable descartar las dimensiones que no se requieran de manera intuitiva. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

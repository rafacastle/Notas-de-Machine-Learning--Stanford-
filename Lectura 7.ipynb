{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea802383",
   "metadata": {},
   "source": [
    "# Lectura 7\n",
    "\n",
    "## RafaCastle\n",
    "\n",
    "## 7.1 Regularización - El Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4aa3128",
   "metadata": {},
   "source": [
    "Consideremos los siguientes ajustes para el mismo conjunto de datos:\n",
    "\n",
    "![Title](Imágenes/7-1-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1368e53f",
   "metadata": {},
   "source": [
    "Es claro que el primer ajuste tienen un error alto en cuanto a la predicción de los datos, por lo que un ajuste lineal se consideraría \"underfit\". El segundo ajuste muestra una buena predicción, cometiendo algunos errores pero tomando una forma bastante adecuada para el comportamiento de los datos. El tercer ajuste se adapta perfectamente a los datos, pero es muy probable que al meter un dato extra, el ajuste de un error muy grande. \n",
    "\n",
    "Por lo tanto un modelo que se ajuste demasiado bien a los datos de entrenamiento, es decir:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta (x^{(i)}) - y^{(i)})^2 \\approx 0\n",
    "$$\n",
    "\n",
    "probablemente comenta errores al probarlo con los datos de prueba. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1e7950",
   "metadata": {},
   "source": [
    "Lo mismo ocurre con un problema de clasificación, consideremos los siguientes conjuntos de datos:\n",
    "\n",
    "![Title](Imágenes/7-1-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0518e738",
   "metadata": {},
   "source": [
    "Igualmente se tiene un modelo con \"underfiting\" a la izquierda, un modelo que se adapta de manera general en el centro y un modelo con \"overfitting\" a la derecha, tratando \"demasiado\" en ajustar el modelo sin fallar, por lo que un dato nuevo probablemente quede fuera del rango con la desición más acertada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5990b2f",
   "metadata": {},
   "source": [
    "Los ejemplos anteriores mostraban un método gráfico para escoger a la mejor función hipótesis, sin embargo esto no siempre es posible. Algunos conjuntos de datos se componen de muchos atributos y deja de ser posible graficar al conjunto de datos. Para resolver este problema se tienen 2 opciones:\n",
    "\n",
    " 1. Reducir el número de atributos\n",
    "  - Manualmente\n",
    "  - Algoritmos de selección de modelos (el algoritmo decide que atributos eliminar)\n",
    " 2. Regularización \n",
    "  - Conservar todos los atributos, pero reducir la magnitud de los parámetros $\\theta_j$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4ffb30",
   "metadata": {},
   "source": [
    "## 7.2 Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece06c8a",
   "metadata": {},
   "source": [
    "Consideremos la primera gráfica de la sección $7.1$, regularizar a la función de la derecha, deberíamos hacer más pequeños a los parámetros $\\theta_3$ y $\\theta_4$, para obtener una ecuación cuadrática con valores más similares a la gráfica del centro y por ende más precisa a la hora de meter un conjunto de datos de prueba.\n",
    "\n",
    "La regularización consta de sumar un nuevo término en la ecuación para $J(\\theta)$ de modo que la función quede descrita por:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m} \\left[ \\sum_{i=1}^m (h_\\theta (x^{(i)}) - y^{(i)})^2 + \\lambda \\sum_{j=1}^n \\theta_j^2 \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5be8bd1",
   "metadata": {},
   "source": [
    "Donde $\\lambda$ es llamado el parámetro de regularización y en general no debe tomar valores muy altos. Parece contra intuitivo imaginar que al sumar dicho término el modelo se volvera más preciso pero en las siguientes secciones se analizarán los casos a fondo para desmenuzar el concepto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd8a21c",
   "metadata": {},
   "source": [
    "## 7.3 Regresión lineal regularizada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bd3a50",
   "metadata": {},
   "source": [
    "El algoritmo gradient descent toma la forma:\n",
    "\n",
    "$$\n",
    "\\theta_o := \\theta_o - \\alpha \\frac{1}{m} \\sum_{i=1}^m (h_\\theta (x^{(i)}) - y^{(i)})x_o^{(i)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha \\frac{1}{m}  \\sum_{i=1}^m (h_\\theta (x^{(i)}) - y^{(i)})x_j^{(i)} -\\alpha \\frac{\\lambda}{m} \\theta_j \n",
    "$$\n",
    "\n",
    "Esta expresión puede reescribirse como:\n",
    "\n",
    "$$\n",
    "\\theta_j := \\theta_j \\left( 1 - \\alpha \\frac{\\lambda}{m} \\right) - \\alpha \\frac{1}{m}  \\sum_{i=1}^m (h_\\theta (x^{(i)}) - y^{(i)})x_j^{(i)} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a75a8cd",
   "metadata": {},
   "source": [
    "El término $1 - \\alpha \\frac{\\lambda}{m}$ usualmente es un número menor a 1. Por lo que el producto $\\theta_j\\left( 1 - \\alpha \\frac{\\lambda}{m} \\right)$ es menor a $\\theta_j$, por lo tanto se está minimizando aún más al nuevo valor $\\theta_j^2$. De esta forma los valores $\\theta_j$ se vuelven más pequeños y se efectúa la regularización."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b766af",
   "metadata": {},
   "source": [
    "Además del algoritmo gradient descent, tambíen es posible usar el algoritmo basado en la ecuación normal para el modelo de regresión lineal. Donde las matrices $X \\in R^{m \\times (n+1)}$ y $y \\in R^m$ están dadas por:\n",
    "\n",
    "$$\n",
    "X = \\left [\n",
    "  \\begin{array}{c}\n",
    "    (x^{(1)})^T \\\\\n",
    "    .\\\\\n",
    "    .\\\\\n",
    "    .\\\\\n",
    "    (x^{(m)})^T\n",
    "  \\end{array}\n",
    "\\right ]\n",
    "$$\n",
    "\n",
    "$$\n",
    "y = \\left [\n",
    "  \\begin{array}{c}\n",
    "    y^{(1)} \\\\\n",
    "    .\\\\\n",
    "    .\\\\\n",
    "    .\\\\\n",
    "    y^{(m)}\n",
    "  \\end{array}\n",
    "\\right ]\n",
    "$$\n",
    "\n",
    "Y la función para hallar los valores mínimos de $\\theta$ tal que $J(\\theta)$ es mínimo está dada (usando regularización) por:\n",
    "\n",
    "$$\n",
    "\\theta = (X^T X + \\lambda M)^{-1} X^T y\n",
    "$$\n",
    "\n",
    "con $M \\in R^{(n+1) \\times (n+1)}$\n",
    "\n",
    "$$\n",
    "M = \\left [\n",
    "  \\begin{array}{cccccc}\n",
    "    0 & 0 & 0 & . . . & 0 & 0  \\\\\n",
    "    0 & 1 & 0 & . . . & 0 & 0  \\\\\n",
    "    0 & 0 & 1 & . . . & 0 & 0  \\\\\n",
    "    &  &  & .   &  &   \\\\\n",
    "    &  &  & .   &  &   \\\\\n",
    "    &  &  & .   &  &   \\\\\n",
    "    0 & 0 & 0 & . . . & 1 & 0  \\\\\n",
    "    0 & 0 & 0 & . . . & 0 & 1  \\\\\n",
    "  \\end{array}\n",
    "\\right ]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e116c21",
   "metadata": {},
   "source": [
    "Es posible demostrar matemáticamente que la matriz $(X^T X + \\lambda M)$ es siempre invertible por lo que no existen excepciones para un caso regularizado en el que este algoritmo no sea válido."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914ed9dd",
   "metadata": {},
   "source": [
    "## 7.4 Regresión logística regularizada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5b30f9",
   "metadata": {},
   "source": [
    "Para aplicar la regularización a la regresión logística, se obtiene la siguiente expresión para la función $J (\\theta)$:\n",
    "\n",
    "$$\n",
    "J(\\theta) = - \\frac{1}{m} \\left[ \\sum_{i=1}^{m} y^{(i)} \\log (h_\\theta (x^{(i)})) + (1-y^{(i)}) \\log(1-h_\\theta (x^{(i)})) \\right] + \\frac{\\lambda}{2m} \\sum_{j=1}^n \\theta_j^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f186017",
   "metadata": {},
   "source": [
    "análogamente el algoritmo toma la forma:\n",
    "\n",
    "$$\n",
    "\\theta_o := \\theta_o - \\alpha \\frac{1}{m} \\sum_{i=1}^m (h_\\theta (x^{(i)}) - y^{(i)})x_o^{(i)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta_j := \\theta_j \\left( 1 - \\alpha \\frac{\\lambda}{m} \\right) - \\alpha \\frac{1}{m}  \\sum_{i=1}^m (h_\\theta (x^{(i)}) - y^{(i)})x_j^{(i)} \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a61834",
   "metadata": {},
   "source": [
    "Que parece idéntico al algorimto para la regresión lineal, claramente la función hipótiesis es diferente para la regresión logística:\n",
    "\n",
    "$$\n",
    "h_\\theta (x) = \\frac{1}{1+ e^{-\\theta^Tx}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498cca02",
   "metadata": {},
   "source": [
    "Si se desea utilizar algín algorimto de optimización avanzada se debe considerar el nuevo factor en la función $J(\\theta)$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "211361ff",
   "metadata": {},
   "source": [
    "# Lectura 12\n",
    "## RafaCastle\n",
    "## 12.1 Support Vector Machines - Objetivo de optimización"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce6418a",
   "metadata": {},
   "source": [
    "El algoritmo de aprendizaje supervizado SVM a veces puede ser más limpio y más poderoso que los algorimos de regresión logística o de redes neuronales a la hora de aprender funciones no lineales complejas.\n",
    "\n",
    "Para describir el funcionamiento de SVM es útil partir de la regresión logística:\n",
    "\n",
    "![Title](Imágenes/12-1-1.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89e9620",
   "metadata": {},
   "source": [
    "En este caso si se tiene $y = 1$, entonces se buscaría que $h_\\theta (x) \\approx 1$, para esto sería necesario $\\theta^T x\\gg 0$. Análogamente si  $y = 0$, entonces $h_\\theta (x) \\approx 0$, para lo cual se requeriría $0 \\gg \\theta^T x$.\n",
    "\n",
    "Al observar a la cost function de la regresión logística, se encuentra que cada ejemplo $(x,y)$ contribuye un término de la forma:\n",
    "\n",
    "$$\n",
    "-(y\\log h_\\theta (x) + (1-y)\\log(1-h_\\theta (x)))\n",
    "= -y\\log \\left( \\frac{1}{1+e^{-\\theta^Tx}} \\right) - (1-y)\\log \\left(1 + \\frac{1}{1+e^{-\\theta^Tx}} \\right)\n",
    "$$\n",
    "\n",
    "Regresando entonces a la condición $y=1$, el segundo término de la expresión anterior se anula y se tiene que, si $z = \\theta^T x$:\n",
    "\n",
    "![Title](Imágenes/12-1-2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d620b7",
   "metadata": {},
   "source": [
    "Así, vemos que para valores a la derecha de la gráfica, la función va dando un aporte muy pequeño, por lo que, en el caso de la regresión logística, si se tiene $y=1$, el algoritmo busca valores de $z$ muy elevados. \n",
    "\n",
    "Ahora, para el algoritmo SVM vamos a tomar esta cost function y la vamos a modificar un poco, de modo que para $z>=1$, la función sea cero y para $z<1$ la función crezca linealmente hacia atrás, como se muestra en la siguiente gráfica.\n",
    "\n",
    "![Title](Imágenes/12-1-3.png)\n",
    "\n",
    "Se hace lo mismo para la segunda expresión de cost function y se denomina a estas expresiones $Cost_o (z)$ y $Cost_1 (z)$ respectivamente. De esta forma, la cost function para SVM es de la forma:\n",
    "\n",
    "$$\n",
    "J (\\theta ) = \\frac{1}{m} \\sum_{i=1}^m [ y^{(i)}Cost_1(\\theta^T x^{(i)}) + (1-y^{(i)})Cost_o(\\theta^T x^{(i)}) ] + \\frac{\\lambda}{2m}\\sum_{j=0}^n \\theta_j^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bf2f21",
   "metadata": {},
   "source": [
    "A diferencia de la regresión logística, SVM no regresa una probabilidad, en lugar de eso se tiene la cost function, la cual da los parámetros al minimizarla y predice de la siguiente forma:\n",
    "\n",
    "$$\n",
    "h_\\theta (x) = \\left\\{ \\begin{array}{cc}\n",
    "1 & \\text{ si } \\theta^Tx \\ge 0 \\\\\n",
    "0 & \\text{ si } \\theta^Tx < 0\n",
    "\\end{array} \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f83cd35",
   "metadata": {},
   "source": [
    "## 12.2 Clasificadores de gran margen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3a88ce",
   "metadata": {},
   "source": [
    "Tomemos la siguiente gráfica de la cost function del algoritmo SVM:\n",
    "\n",
    "![Title](Imágenes/12-1-4.png)\n",
    "\n",
    "Donde $C = \\frac{1}{\\lambda}$, en este caso, si $y=1$ no solo queremos $z\\ge 0$ sino $z \\ge 1$ y para $y=0$ $z \\le -1$. Esto se hace para definir un margen mayor al momento de decidir la predicción mediante el algoritmo SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d4a0df",
   "metadata": {},
   "source": [
    "Las consecuencias de esta corrección en el algoritmo SVM son sumamente útiles, al hacer esta corrección, el algoritmo puede definir una frontera entre 2 categorías de datos con un margen mayor que otros algorimtos, ya que busca el máximo margen posible, como se muestra en la figura:\n",
    "\n",
    "![Title](Imágenes/12-1-5.png)\n",
    "\n",
    "Es por esto que al algoritmo SVM se le denomina un clasificador de gran margen. En la siguiente sección se explicará como es que esta corrección en la desigualdad conduce a estos resultados al momento de implementar el algoritmo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a61bbd",
   "metadata": {},
   "source": [
    "Cabe resaltar que el algoritmo puede ser muy sensible a outliers si se toma un valor de $C$ muy grande, al tomar un valor pequeño de $C$ se reduce la sensibilidad del algorimto a cambiar abruptamente su desición debido a un outlier, como se muestra en la figura:\n",
    "\n",
    "![Title](Imágenes/12-1-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a8759e",
   "metadata": {},
   "source": [
    "## 12.3 Las matemáticas detrás del SVM (opcional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd9f0c6",
   "metadata": {},
   "source": [
    "Esta sección está marcada como opcional en el curso ([Link al vídeo](https://www.youtube.com/watch?v=QKc3Tr7U4Xc&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=72)). Se tratan los siguientes temas:\n",
    "\n",
    "1. Repaso de álgebra lineal - Producto interno de vectores (inicio - 5:57)\n",
    "2. Producto interno en SVM (5:57 - 11:21) \n",
    "3. Límite de desición de SVM (11:21 - final) \n",
    "\n",
    "Los primeros 2 puntos pueden saltarse si se tienen bases sólidas en álgebra lineal, sin embargo, recomiendo altamente ver el punto 3 ya que clarifica como la corrección mencionada en la sección anterior hace que el algoritmo SVM escoga la frontera más conveniente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26aa6f5",
   "metadata": {},
   "source": [
    "## 12.4 Kernels I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c84e555",
   "metadata": {},
   "source": [
    "La técnica para clasificar conjuntos de datos mediante fronteras no lineales es denominado kernels. Si se tiene un conjunto de prueba como el que se muestra a continuación:\n",
    "\n",
    "![Title](Imágenes/12-4-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59af53f5",
   "metadata": {},
   "source": [
    "Se busca predecir $y = 1$ si $\\theta_o + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_1x_2 + \\theta_4 x_1^2 + \\theta_5 x_2^2 + ... \\ge 0 $. por lo que la función hipótesis tendría la forma:\n",
    "\n",
    "$$\n",
    "h_\\theta (x) = \\left\\{ \\begin{array}{cc}\n",
    "1 & \\text{ si } \\theta_o + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_1x_2 + \\theta_4 x_1^2 + \\theta_5 x_2^2 + ... \\ge 0  \\\\\n",
    "0 & \\text{ si } \\theta_o + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_1x_2 + \\theta_4 x_1^2 + \\theta_5 x_2^2 + ... < 0\n",
    "\\end{array} \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e51ba2",
   "metadata": {},
   "source": [
    "Para simplificar la notación puede tomarse $f_1 = x_1$, $f_2 = x_2$, $f_3 = x_1x_2$, $f_4 = x_1^2$, etc. La pregunta sería: ¿Existe una mejor manera de escoger los términos $f_i$? Para responder a esta pregunta tomemos el siguiente ejemplo:\n",
    "\n",
    "Definamos 3 nuevas variables $l^(1),l^(2),l^(3)$ y tal que:\n",
    "\n",
    "![Title](Imágenes/12-4-2.png)\n",
    "\n",
    "Así, podemos dar un punto $x$ de modo que los términos $f_i$ cumplan:\n",
    "\n",
    "$$\n",
    "f_i = \\text{Similaridad} (x,l^{(i)}) = \\exp \\left(- \\frac{||x - l^{(i)}||^2}{2 \\sigma^2} \\right)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4d740f",
   "metadata": {},
   "source": [
    "El término matemático para la función similaridad es \"Kernel\" y en este caso en particular se trata de un \"kernel gaussiano\". Denotemos a esta función como $k(x,l^{(i)})$. \n",
    "\n",
    "Veamos ahora qué es lo que esta función está haciendo.\n",
    "\n",
    "$$\n",
    "f_i = k(x,l^{(i)}) = \\exp \\left(- \\frac{||x - l^{(i)}||^2}{2 \\sigma^2} \\right) = \\exp \\left(- \\frac{\\sum_{j=1}^n (x_j - l_j^{(i)})^2}{2 \\sigma^2} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affdc853",
   "metadata": {},
   "source": [
    "Si se tiene el caso $x \\approx l^{(i)}$ entonces $f_i \\approx \\exp \\left(- \\frac{0^2}{2 \\sigma^2} \\right) \\approx 1$. Por otro lado si $x$ está lejos de  $l^{(i)}$ entonces $f_i \\approx \\exp \\left(- \\frac{\\text{número muy grande}^2}{2 \\sigma^2} \\right) \\approx 0$.\n",
    "\n",
    "Como acción ilustrativa, tomemos el siguiente ejemplo a modo de ilustrar, demosle un cierto valor al vector $l^{(1)}$ y variemos los valores de las entradas del vector $x$:\n",
    "\n",
    "![Title](Imágenes/12-4-3.png)\n",
    "\n",
    "Veamos que ocurre si se varía el valor de $\\sigma^2$\n",
    "\n",
    "![Title](Imágenes/12-4-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a925be81",
   "metadata": {},
   "source": [
    "Es posible ver que la función decae más lento del punto $l^{(1)}$ si se incrementa el valor de $\\sigma^2$. \n",
    "\n",
    "Ahora, notemos lo siguiente, supongamos que $\\theta_o = -0.5, \\theta_1 = 1,\\theta_2 = 1,\\theta_3 = 0$ y se se busca predecir $h_\\theta = 1$ cuando $\\theta_o + theta_1x_1 + theta_2x_2 + theta_3x_3 \\ge 0$ para los puntos en la siguiente gráfica.\n",
    "\n",
    "![Title](Imágenes/12-4-5.png)\n",
    "\n",
    "En dicho caso, es claro que para $x'_1$ el algoritmo va a predecir $h_\\theta = 1$, ya que $f_1 \\approx 1, f_3 \\approx 0,f_3 \\approx 0$. Sin embargo, para $x'_2$ el algoritmo va a predecir $h_\\theta = 0$, ya que $f_1 \\approx 0, f_3 \\approx 0,f_3 \\approx 0$. Por lo tanto hemos obtenido un algoritmo que va a predecir $y=1$ para un punto $x$ cuando $x$ se encuentre relativamente cerca de alguno de los puntos $l$ (para este caso en particular no se requiere que $x$ este cerca del punto $l^{(3)}$ ya que $\\theta_3 = 0$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12e9029",
   "metadata": {},
   "source": [
    "## 15.5 Kernels II"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0750fae",
   "metadata": {},
   "source": [
    "Veamos ahora como obtener los puntos $l^{(1)}, l^{(2)} y l^{(3)}$. Lo primero es tomar un conjunto de datos como el que se muestra a la izquierda en la siguiente figura y asignar los vectores $l$ de acuerdo a dichos datos, como se muestra a la derecha de la figura:\n",
    "\n",
    "![Title](Imágenes/12-5-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11355cd",
   "metadata": {},
   "source": [
    "de esta manera terminaremos con $m$ puntos $l$ si hay $m$ datos en el conjunto de entrenamiento. Dicho de otra forma, dado $(x^{(1)},y^{(1)})$, $(x^{(2)},y^{(2)})$, ... ,$(x^{(m)},y^{(m)})$ escogeremos $l^{(i)} = x^{(i)}$ para $i \\le m$. Y los términos $f$ temdran la forma $f_i = k(x,l^{(i)})$ con $x$ un vector dado, tomando siempre $f_o = 1$. Así,\n",
    "\n",
    "$$\n",
    "f_j^{(i)} = k (x^{(i)}, l^{(i)})\n",
    "$$\n",
    "\n",
    "Con $j = 1,...,m$ y $x^{(i)} \\in R^{n+1}$ recordando que $f_o^{(i)} = 1$. Con esto podemos plantear la siguiente hipótesis:\n",
    "\n",
    "Dado $x$, los términos $f$ predecirán $y = 1$ si $\\theta^T f = \\theta_o f_o + \\theta_1 f_1 + ... + \\theta_m f_m\\ge 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1229104b",
   "metadata": {},
   "source": [
    "Por lo tanto en el algoritmo SVM se calcula al mínimo de la función $J$ como:\n",
    "$$\n",
    "\\min \\frac{1}{m} \\sum_{i=1}^m [ y^{(i)}Cost_1(\\theta^T f^{(i)}) + (1-y^{(i)})Cost_o(\\theta^T f^{(i)}) ] + \\frac{\\lambda}{2m}\\sum_{j=0}^n \\theta_j^2\n",
    "$$\n",
    "\n",
    "A diferencia del algoritmo de regresión donde se tiene la diferencia $\\theta^T f^{(i)} \\to \\theta^T x^{(i)}$. Además para este algoritmo se tiene $n=m$ ya que se tienen $m+1$ parámetros $\\theta_j$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510a51b5",
   "metadata": {},
   "source": [
    "## 12.6 Usando el algoritmo SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c87468",
   "metadata": {},
   "source": [
    "En una situación real no es recomendado desarrollar un algoritmo propio para usar el SVM, se recomienda usar un software ya creado dado el costo computacional. Para usarlo, generalmente se deberán otorgar ciertos parámtros como $C$ o el tipo de kernel que debe usarse (gaussiano, lineal, etc). \n",
    "\n",
    "Algunos paquetes SVM tienen incluida la funcionalidad de clasificar multiples clases. El método en el que SVM clasifica estos casos es similar al de la regresión lineal \"1 contra todos\". Donde solo se distingue a una clase de las demás $y=1$, luego se toma a otra clase diferente y se distingue a esa $y=2$ y así respectivamente.\n",
    "\n",
    "Finalmente, vale la pena discutir cuando es conveniente usar regresión logística y cuando SVM. Para seccionar los casos, consideremos un conjunto con $n$ atributos y $m$ datos.\n",
    "\n",
    "1. Si $n >> m$ se recomienda usar regresión logística o SVM con un kernel lineal.\n",
    "2. Si $n \\approx m$ recomienda usar SVM con un kernel gaussiano. \n",
    "3. Si $n << m$ se recomienda crear más atributos y después usar regresión logística con un kernel lineal. Una red neuronal podría ser útil pero tomará más tiempo entrenarla."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
